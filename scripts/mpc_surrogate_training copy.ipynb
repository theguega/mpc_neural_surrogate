{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPC Surrogate Training Pipeline\n",
    "\n",
    "This notebook implements the complete training and evaluation pipeline for approximating MPC policies using neural networks.\n",
    "\n",
    "## Dataset Structure\n",
    "- **States**: Joint positions and velocities (6D) - [q1, q2, q3, q̇1, q̇2, q̇3]\n",
    "- **Targets**: End-effector target positions (3D) - [x, y, z]\n",
    "- **Actions**: MPC torques (3D) - [τ1, τ2, τ3]\n",
    "\n",
    "The goal is to learn a mapping: (state, target) → MPC torques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install h5py numpy matplotlib scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation - Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPCDataset(Dataset):\n",
    "    def __init__(self, states, targets, actions, augment=False):\n",
    "        self.states = torch.FloatTensor(states)\n",
    "        self.targets = torch.FloatTensor(targets)\n",
    "        self.actions = torch.FloatTensor(actions)\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        state = self.states[idx]\n",
    "        target = self.targets[idx]\n",
    "        action = self.actions[idx]\n",
    "\n",
    "        if self.augment:\n",
    "            # Simple data augmentation: add small noise to states and targets\n",
    "            state_noise = torch.randn_like(state) * 0.01  # 1% noise\n",
    "            target_noise = torch.randn_like(target) * 0.005  # 0.5% noise\n",
    "            state = state + state_noise\n",
    "            target = target + target_noise\n",
    "\n",
    "        # Concatenate state and target as input\n",
    "        x = torch.cat([state, target], dim=0)\n",
    "        return x, action\n",
    "\n",
    "def load_data(filename='robot_mpc_dataset.h5'):\n",
    "    \"\"\"Load data from HDF5 file\"\"\"\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        states = f['states'][:]\n",
    "        targets = f['targets'][:]\n",
    "        actions = f['actions'][:]\n",
    "    return states, targets, actions\n",
    "\n",
    "def create_dataloaders(states, targets, actions, batch_size=32, train_ratio=0.8, augment=True):\n",
    "    \"\"\"Create train and test dataloaders\"\"\"\n",
    "    dataset = MPCDataset(states, targets, actions, augment=augment)\n",
    "\n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Disable augmentation for test set\n",
    "    test_dataset.dataset.augment = False\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Load and prepare data\n",
    "states, targets, actions = load_data('data/robot_mpc_dataset.h5')\n",
    "print(f\"Dataset shapes:\")\n",
    "print(f\"States: {states.shape}\")\n",
    "print(f\"Targets: {targets.shape}\")\n",
    "print(f\"Actions: {actions.shape}\")\n",
    "\n",
    "train_loader, test_loader = create_dataloaders(states, targets, actions, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_episode_indices(targets, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Splits the dataset into episodes by detecting when target changes.\n",
    "    Assumes each episode has a constant target_xyz.\n",
    "    \"\"\"\n",
    "    episode_starts = [0]\n",
    "    for i in range(1, len(targets)):\n",
    "        if np.linalg.norm(targets[i] - targets[i - 1]) > tol:\n",
    "            episode_starts.append(i)\n",
    "    episode_starts.append(len(targets))\n",
    "    return episode_starts\n",
    "\n",
    "\n",
    "def visualize_random_episode(h5_path=\"data/robot_mpc_dataset.h5\"):\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        states = f[\"states\"][:]  # (N, 6)\n",
    "        targets = f[\"targets\"][:]  # (N, 3)\n",
    "        actions = f[\"actions\"][:]  # (N, 3)\n",
    "\n",
    "    # Find where episodes split\n",
    "    episode_boundaries = load_episode_indices(targets)\n",
    "    num_episodes = len(episode_boundaries) - 1\n",
    "\n",
    "    print(f\"Found {num_episodes} episodes in dataset.\")\n",
    "\n",
    "    # Sample a random episode\n",
    "    ep_idx = np.random.randint(num_episodes)\n",
    "    start = episode_boundaries[ep_idx]\n",
    "    end = episode_boundaries[ep_idx + 1]\n",
    "\n",
    "    ep_states = states[start:end]\n",
    "    ep_targets = targets[start:end]\n",
    "    ep_actions = actions[start:end]\n",
    "\n",
    "    print(f\"Visualizing episode {ep_idx} ({end - start} timesteps).\")\n",
    "    print(f\"Target = {ep_targets[0]}\")\n",
    "\n",
    "    # Plot joint positions (first 3 states)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(ep_states[:, 0], label=\"q1\")\n",
    "    plt.plot(ep_states[:, 1], label=\"q2\")\n",
    "    plt.plot(ep_states[:, 2], label=\"q3\")\n",
    "    plt.title(\"Joint Positions\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot joint velocities (last 3 states)\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(ep_states[:, 3], label=\"dq1\")\n",
    "    plt.plot(ep_states[:, 4], label=\"dq2\")\n",
    "    plt.plot(ep_states[:, 5], label=\"dq3\")\n",
    "    plt.title(\"Joint Velocities\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot MPC actions\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(ep_actions[:, 0], label=\"tau1\")\n",
    "    plt.plot(ep_actions[:, 1], label=\"tau2\")\n",
    "    plt.plot(ep_actions[:, 2], label=\"tau3\")\n",
    "    plt.title(\"MPC Torques\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple Multi-Layer Perceptron\"\"\"\n",
    "    def __init__(self, input_dim=9, hidden_dims=[128, 64], output_dim=3):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.Dropout(0.1)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.linear1 = nn.Linear(dim, dim)\n",
    "        self.linear2 = nn.Linear(dim, dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.linear1(x))\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear2(out)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"Residual Network for MPC surrogate\"\"\"\n",
    "    def __init__(self, input_dim=9, hidden_dim=128, num_blocks=3, output_dim=3):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.res_blocks = nn.ModuleList([ResidualBlock(hidden_dim) for _ in range(num_blocks)])\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.input_layer(x))\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4, ff_dim=None):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        if ff_dim is None:\n",
    "            ff_dim = 4 * dim\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(ff_dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "\n",
    "        # Feed-forward\n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformer-based MPC surrogate\"\"\"\n",
    "    def __init__(self, input_dim=9, embed_dim=64, num_heads=4, num_blocks=2, output_dim=3):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_embedding = nn.Linear(input_dim, embed_dim)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads) for _ in range(num_blocks)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(embed_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add batch and sequence dimensions for transformer\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)  # [batch, 1, features]\n",
    "\n",
    "        x = self.input_embedding(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "        x = x.squeeze(1)  # Remove sequence dimension\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Model configurations\n",
    "model_configs = {\n",
    "    'MLP': {'class': MLP, 'params': {'hidden_dims': [128, 64]}},\n",
    "    'ResNet': {'class': ResNet, 'params': {'hidden_dim': 128, 'num_blocks': 3}},\n",
    "    'Transformer': {'class': Transformer, 'params': {'embed_dim': 64, 'num_heads': 4, 'num_blocks': 2}}\n",
    "}\n",
    "\n",
    "print(\"Available models:\", list(model_configs.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            all_predictions.append(outputs.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    predictions = np.concatenate(all_predictions)\n",
    "    targets = np.concatenate(all_targets)\n",
    "\n",
    "    mse = mean_squared_error(targets, predictions)\n",
    "    mae = mean_absolute_error(targets, predictions)\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / num_batches,\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'predictions': predictions,\n",
    "        'targets': targets\n",
    "    }\n",
    "\n",
    "def train_model(model_name, model, train_loader, test_loader, num_epochs=100, patience=10):\n",
    "    \"\"\"Complete training pipeline for a model\"\"\"\n",
    "    print(f\"\\n=== Training {model_name} ===\")\n",
    "\n",
    "    # Loss functions\n",
    "    mse_criterion = nn.MSELoss()\n",
    "    mae_criterion = nn.L1Loss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "    # Schedulers\n",
    "    mse_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    mae_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'hyperparameters': {\n",
    "            'lr': 1e-3,\n",
    "            'weight_decay': 1e-4,\n",
    "            'batch_size': train_loader.batch_size,\n",
    "            'num_epochs': num_epochs\n",
    "        },\n",
    "        'training_history': [],\n",
    "        'best_mse_results': None,\n",
    "        'best_mae_results': None\n",
    "    }\n",
    "\n",
    "    best_mse_loss = float('inf')\n",
    "    best_mae_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train with MSE\n",
    "        train_loss_mse = train_epoch(model, train_loader, mse_criterion, optimizer, device)\n",
    "        mse_results = evaluate(model, test_loader, mse_criterion, device)\n",
    "\n",
    "        # Train with MAE\n",
    "        train_loss_mae = train_epoch(model, train_loader, mae_criterion, optimizer, device)\n",
    "        mae_results = evaluate(model, test_loader, mae_criterion, device)\n",
    "\n",
    "        # Update schedulers\n",
    "        mse_scheduler.step(mse_results['loss'])\n",
    "        mae_scheduler.step(mae_results['loss'])\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        # Log results\n",
    "        epoch_results = {\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss_mse': train_loss_mse,\n",
    "            'test_loss_mse': mse_results['loss'],\n",
    "            'test_mse': mse_results['mse'],\n",
    "            'test_mae': mse_results['mae'],\n",
    "            'train_loss_mae': train_loss_mae,\n",
    "            'test_loss_mae': mae_results['loss'],\n",
    "            'epoch_time': epoch_time\n",
    "        }\n",
    "        results['training_history'].append(epoch_results)\n",
    "\n",
    "        # Save best models\n",
    "        if mse_results['loss'] < best_mse_loss:\n",
    "            best_mse_loss = mse_results['loss']\n",
    "            results['best_mse_results'] = mse_results.copy()\n",
    "            torch.save(model.state_dict(), f'{model_name}_best_mse.pth')\n",
    "\n",
    "        if mae_results['loss'] < best_mae_loss:\n",
    "            best_mae_loss = mae_results['loss']\n",
    "            results['best_mae_results'] = mae_results.copy()\n",
    "            torch.save(model.state_dict(), f'{model_name}_best_mae.pth')\n",
    "\n",
    "        # Early stopping\n",
    "        if mse_results['loss'] >= best_mse_loss:\n",
    "            patience_counter += 1\n",
    "        else:\n",
    "            patience_counter = 0\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, MSE Loss: {mse_results['loss']:.6f}, MAE Loss: {mae_results['loss']:.6f}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "all_results = {}\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    # Create model\n",
    "    model_class = config['class']\n",
    "    model_params = config['params']\n",
    "    model = model_class(**model_params).to(device)\n",
    "\n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\n{model_name} - Parameters: {total_params:,}\")\n",
    "\n",
    "    # Train model\n",
    "    results = train_model(model_name, model, train_loader, test_loader, num_epochs=50)\n",
    "    all_results[model_name] = results\n",
    "\n",
    "print(\"\\n=== Training Complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# Export comprehensive results\n",
    "for model_name, results in all_results.items():\n",
    "    # Save detailed results as JSON\n",
    "    results_file = f'results/{model_name}_results.json'\n",
    "    with open(results_file, 'w') as f:\n",
    "        # Convert numpy arrays to lists for JSON serialization\n",
    "        json_results = results.copy()\n",
    "        if 'best_mse_results' in json_results and json_results['best_mse_results']:\n",
    "            json_results['best_mse_results'].pop('predictions', None)\n",
    "            json_results['best_mse_results'].pop('targets', None)\n",
    "        if 'best_mae_results' in json_results and json_results['best_mae_results']:\n",
    "            json_results['best_mae_results'].pop('predictions', None)\n",
    "            json_results['best_mae_results'].pop('targets', None)\n",
    "        json.dump(json_results, f, indent=2)\n",
    "\n",
    "    print(f\"Results saved to: {results_file}\")\n",
    "\n",
    "# Export summary table\n",
    "summary_data = []\n",
    "for model_name, results in all_results.items():\n",
    "    row = {\n",
    "        'model_type': model_name,\n",
    "        'hyperparameters': results['hyperparameters'],\n",
    "        'best_mse_loss': results.get('best_mse_results', {}).get('loss', 'N/A'),\n",
    "        'best_mae_loss': results.get('best_mae_results', {}).get('loss', 'N/A'),\n",
    "        'best_mse_mse': results.get('best_mse_results', {}).get('mse', 'N/A'),\n",
    "        'best_mae_mae': results.get('best_mae_results', {}).get('mae', 'N/A'),\n",
    "        'epochs_trained': len(results['training_history'])\n",
    "    }\n",
    "    summary_data.append(row)\n",
    "\n",
    "with open('results/training_summary.json', 'w') as f:\n",
    "    json.dump(summary_data, f, indent=2)\n",
    "\n",
    "print(\"Summary saved to: results/training_summary.json\")\n",
    "\n",
    "# Export model weights (already saved during training)\n",
    "print(\"Model weights saved as .pth files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, (model_name, results) in enumerate(all_results.items()):\n",
    "    history = results['training_history']\n",
    "\n",
    "    epochs = [h['epoch'] for h in history]\n",
    "    mse_losses = [h['test_loss_mse'] for h in history]\n",
    "    mae_losses = [h['test_loss_mae'] for h in history]\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, mse_losses, label=model_name)\n",
    "    plt.title('MSE Loss vs Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, mae_losses, label=model_name)\n",
    "    plt.title('MAE Loss vs Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE Loss')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print final results summary\n",
    "print(\"\\n=== Final Results Summary ===\")\n",
    "print(f\"{'Model':<12} {'MSE Loss':<10} {'MAE Loss':<10} {'MSE':<10} {'MAE':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for model_name, results in all_results.items():\n",
    "    mse_results = results.get('best_mse_results', {})\n",
    "    mae_results = results.get('best_mae_results', {})\n",
    "\n",
    "    mse_loss = f\"{mse_results.get('loss', 'N/A'):.6f}\" if mse_results else 'N/A'\n",
    "    mae_loss = f\"{mae_results.get('loss', 'N/A'):.6f}\" if mae_results else 'N/A'\n",
    "    mse_val = f\"{mse_results.get('mse', 'N/A'):.6f}\" if mse_results else 'N/A'\n",
    "    mae_val = f\"{mae_results.get('mae', 'N/A'):.6f}\" if mae_results else 'N/A'\n",
    "\n",
    "    print(f\"{model_name:<12} {mse_loss:<10} {mae_loss:<10} {mse_val:<10} {mae_val:<10}\")\n",
    "\n",
    "print(\"\\nModel weights saved as .pth files\")\n",
    "print(\"Detailed results saved in results/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip file for easy download\n",
    "!zip -r mpc_surrogate_results.zip results/ *.pth\n",
    "\n",
    "from google.colab import files\n",
    "files.download('mpc_surrogate_results.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
